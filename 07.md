

Continuous Research Methods
===========================


The need for answers about user needs, preferences, and pain points
doesn't go away after you launch a product---and that means the need for
UX research doesn't go away, either.

\

Things break, new features need to be launched, customer needs evolve. 
All these things require research.

\

Continuous research (or ongoing research) is an important part of
maintaining a great customer experience. Ongoing listening methods like
NPS surveys, user analytics, continuous customer interviews help keep
your product useful, relevant, and valuable to users.

\

In this chapter, you'll learn about:

-   **How to use product analytics** like in-app behaviors, user flows,
    dropoff rates, activation rates, and other product metrics to
    support your research and keep track of changes over time.
-   **Continuous feedback surveys**---like net promoter score (NPS),
    customer satisfaction score (CSAT), customer effort score (CES), and
    custom intercept surveys---and how they can help you stay on the
    pulse of user sentiment.
-   **Using** **sales, support, CS, and product data** in user research.
    Support tickets, bug reports, customer quotes, and first-hand
    knowledge of customers can be a goldmine of both qualitative and
    quantitative data if you know how to use it
-   **Continuous user interviews**---or regular, 1-1 meetings with your
    customers---to augment one-off studies with fresh insights,
    prioritize your product roadmap, contribute to agile functioning
    throughout your org, and create a research-first culture on your
    team.

‍

Your product and customer-facing teams are sitting on a mountain of
quantitative data and user feedback. Here's how to tap into it.



User analytics gives you insight into how your product is actually being
used, out in the wild.

‍

Most product teams are already collecting some form of user analytics,
perhaps in the form of passive usage data or website behavior. But what
many teams aren't doing yet is using user analytics data for UX research
too! 

‍

Here, we'll provide concrete, actionable advice for using the user
analytics that teams are ***already*** collecting to answer [user
research
questions](https://www.userinterviews.com/ux-research-field-guide-chapter/user-research-questions)
and support [continuous (qualitative)
research](https://www.userinterviews.com/ux-research-field-guide-module/continuous-research-methods).

‍

### In this chapter:

-   What is user behavior analytics?
-   Why does user analytics matter?
-   Benefits and limitations of user analytics
-   How to get started with user analytics for UX research
-   User analytics tools and software

What is user behavior analytics?
--------------------------------

User analytics (a.k.a user behavior analytics) is a form of continuous,
quantitative data tracking and evaluation that occurs post-launch. Teams
use analytics tools to passively collect data about users' interactions
with their product, app, or website. Then, they analyze this data to
better understand user engagement and sentiment. 

‍

UX researchers and
[PwDRs](https://www.userinterviews.com/blog/people-who-do-research-discovery-study)
(people who do research who aren't part of the core UXR team, such as
marketers, UX designers, product managers, and engineers) can use user
analytics to:

-   Identify issues with the product
-   Investigate hypotheses about design or technical issues
-   Monitor the user journey at key moments like activation
-   Quantify the user experience
-   Target users at specific moments in their journey for user research
-   Persuade data-oriented stakeholders

### The difference between user analytics and UX research

![User analytics vs. UX
research](https://global-uploads.webflow.com/59b1667dd2e65000019d07be/63926817480b8c2961102115_rCMZLLcXplDR34x8PHw80peF1Mpsu2M8ko8YPFUKT5-xcKSA5pdpdIUMj3kXlb6XHghAAoj5McwvCdb-Qr_O4FhVqme3cmUfzwlzQ_TdPubQXTj2P1J1_zzbXpl18SglwV6KcLBu35vyK-7ChlFPprHpe72YSVNCKrdKGqG4Rj05o7KbTwwN4YM8ThcpfA.png)

‍

To understand the difference between user analytics and user research,
it's helpful to review the difference between [qualitative and
quantitative
research](https://www.userinterviews.com/ux-research-field-guide-chapter/qualitative-vs-quantitative-vs-mixed-methods):

-   **Quantitative data** is a measure of numerical values and answers
    questions like "how many" and "how often." It usually involves
    collecting large volumes of data indirectly from analytics tools. 
-   **Qualitative data** is categorical or thematic, taking the form of
    stories, observations, thoughts, motivations, and feelings, and it
    answers questions like "why." Typically, qualitative data is
    gathered through direct observation of a small group of people.

‍

User analytics is quantitative; it gathers numerical data from large
sample sizes, usually passively through tools like Qualtrics or Google
Analytics. User research, on the other hand, is qualitative, digging
into the reasons and motivations behind user behavior. 

‍

As Yaron Cohen, Senior UX researcher at RBC, says in his [article about
collaboration between user researchers and analytics
specialists](https://uxplanet.org/three-ways-ux-researchers-and-analytics-specialists-can-collaborate-and-deliver-great-insights-c85f288e7ea2):

‍

> "Analytics professionals spend their days extracting transactional and
> behavioral data from databases, and analyzing data that was recorded
> by electronic systems and not provided by the customers themselves
> (implicit data)\...\
> \
> UX researchers, on the other hand, work with explicit data provided by
> customers in the form of surveys, interviews, and usability studies."

‍

Despite (and because of) their differences, UX research and analytics
make a great pair. Used together, insights from each can inform holistic
customer personas, pinpoint the causes of churn, and reveal
opportunities to improve retention rates, product usage, and
conversions. 

Why does user analytics matter?
-------------------------------

Quantitative user analytics are important to all organizations, with
different departments and functions monitoring what is most relevant to
their needs. 

‍

For example, the C-Suite is likely watching top-line metrics, like
daily, weekly, and monthly revenue, plus whatever key metrics drive that
revenue. Marketers are focused on metrics that drive revenue throughout
the entire funnel. Product teams are focused on product usage and
user-centric metrics that help drive that usage (and ultimately
revenue).

‍

Whoever the person or team, the beauty of rallying around quantitative
metrics is everyone can speak the same language in a pretty objective
way. In the context of ongoing listening following a product release,
you probably have some historical benchmarks to watch post-launch,
focusing on the areas you were trying to impact. Mapping user goals, to
product goals, to revenue goals, to quantitative metrics is an important
way to align goals across an organization.

‍

In other words, user analytics can help you:

-   Align goals and metrics across the organization
-   Understand actual user behavior with the product
-   Improve product design and development
-   Identify and predict trends 
-   Retain and upsell existing customers 

‍

Note that often, pairing quantitative methods with qualitative methods
like user interviews can reap the best, most well-rounded results.

‍

Benefits and limitations of user analytics
------------------------------------------

Here at User Interviews, we're pretty enthusiastic about all forms of
research, including user analytics. However, like any research method,
user analytics comes with both pros and cons. 

‍

As our own VP of Analytics, Utsav Kaushish, has said in his article,
"[I'm a Data Scientist and Here's Why Quantitative Data Isn't
Enough](https://www.userinterviews.com/blog/im-a-data-scientist-and-heres-why-quantitative-data-isnt-enough)":

‍

> "I know firsthand how valuable analytics can be for a company; I've
> literally made a living from it. But I also know of its limitations;
> there are times when you have to dig deeper than possible with a query
> or a regression model. And often the best way to do this is to simply
> talk to the people you want to use your product. A truly data-driven
> organization will complement their analytics with user research, and
> use learnings from one to power the other."

‍

### Benefits of user analytics include: 

-   **Saves time and cost:** Because user analytics data is collected
    passively (i.e. without a researchers' oversight and independent of
    a research study), it's a cost-efficient source of insight for
    resource-constrained teams. 
-   **Reveals real-world user behavior "outside the lab":** A common
    challenge with research is that users sometimes behave differently
    than they normally would when they know they're partaking in a
    study. User analytics data is collected without users' knowledge,
    painting a more accurate picture of their behavior. 
-   **Access to large sample sizes:** User analytics data, such as web
    browsing behavior, can be collected by a sizable number of users
    quickly, easily, and (relatively) cheaply, allowing for
    statistically significant insights. 
-   **Limited researcher bias:** Because user analytics is quantitative
    data collected passively, the data itself can't be skewed by a
    human's perspective or interference. As always, however, the human
    analysis of this data may be at risk for bias. 
-   **Informs prioritization:** User analytics can help you decide which
    bug fixes, product developments, and research projects to prioritize
    in your roadmap. 

### Limitations of user analytics include:

-   **A lack of information about user motivations and sentiment:** Like
    most quantitative methods, user analytics doesn't tell you the "why"
    behind user behavior, which is important in determining the best way
    to respond to that behavior. 
-   **Risk of making assumptions about the "why" behind user behavior:**
    Because user analytics doesn't tell you "why" users took certain
    actions, the person analyzing the data might introduce bias by
    making assumptions about the "why." This risk can be mitigated by
    pairing user analytics with qualitative research (we'll talk more
    about this in the next section).
-   **Difficulty connecting data with actionable insights:** As Jen
    Cardello said on
    [NN/g](https://www.nngroup.com/articles/analytics-user-experience/):
    "The biggest issue with analytics is that it can very quickly become
    a distracting black hole of 'interesting' data without any
    actionable insight."
-   **Large amounts of data:** While large sample sizes allow you to
    achieve statistical significance, they may also make it difficult to
    differentiate between meaningful data and noise. 
-   **Many analytics systems are not purpose-built for user
    researchers:** Instead, these systems are typically built for
    marketers, product people, and designers---but this is one of the
    reasons we believe it's valuable for UXR to work with these teams!

How to get started with user analytics for UX research
------------------------------------------------------

You're probably already collecting some form of user behavior data at
your organization, in which case, some of these steps will be review for
you:

‍

1.  Start with user goals.
2.  Determine the analytics metrics you want to track.
3.  Set up a system for measuring analytics.
4.  Set a cadence for reviewing analytics.
5.  Identify trends, user segments, and other patterns in the data.
6.  Conduct formalized UX research to explore trends, questions, and
    opportunities.

‍

However, let's start from the beginning for folks who want a refresher.

‍

### 1. Start with user goals.

‍

Begin by understanding your user's goals. 

‍

For example, if you've created an experience in a fitness app for
someone who wants to get lean, you need to understand which actions are
the most important for meeting this goal, then track the ease with which
folks can complete those actions.‍ 

‍

So, how do you know when someone's having trouble? Sometimes users give
you feedback directly through a survey or support interaction. Other
times the writing is on the wall in the form of quantitative
analytics---like Google Analytics for web, Mixpanel for product events,
or other tools in your analytics stack.

‍

### 2. Determine the analytics metrics you want to track.

‍

Depending on the goals of your product launch or feature updates, you
may want to focus on some of the common quantitative metrics below. 

‍

#### Survey analytics

![Fictional NPS data over time via
Promoter.io](https://global-uploads.webflow.com/59b1667dd2e65000019d07be/6392681706dcccbab88e62aa_9M7FxBrRWNy1-cj29ENiJ5YnkUV26D1oDST15xiTjlcGi2JPnDRPRTaLT94WNZb1tQxq0a1o4wXPfnrq2N55CbP_3R99oSf22HL2deVL6Od9F9PMevlbdwOMgj8c_P0pkX1QPjZQgxPwf52x3LHX9UNadsaTnHevzjW3V_oGc0pZAnADyGweNBlpZxhw6g.png)

‍

**NPS, CSAT, or CES scores**

These survey metrics give you an idea of how your customers feel about
your company and particular touchpoints with your company, such as the
support experience. 

‍

If these scores are changing (positively or negatively) following a
launch, this can be an indication of how your changes are being
received. 

‍

Drilling into your data with a focus on key segments and cohorts, and
validating with further data will help you uncover insights.

‍

**Qualitative survey data**

Often the above surveys include a free-form question that can help you
understand some of the whys, the motivations behind the positive or
negative scores. 

‍

As you combine quantitative data with qualitative and segment it by
meaningful customer groups, you should start to form hypotheses you can
then validate through testing and further research.

‍

#### Product analytics

![Example of retention data from
Mixpanel](https://global-uploads.webflow.com/59b1667dd2e65000019d07be/63926818c756aab9e3d9e87b_E-UKtC7tuAJMMkHyNdRCs2RnoiSGIfAtXTBB5uSWi6Wp3Pczk9D1XeBzsCrcpveba9iZE_JNQoSb63Qt7iCe8QL0bOKghhRybPVew8DiQb3A1XjeNVOWRQqJ6J2VBjQWShVhQwlhhbmJC8HK_arcojnmL4Vdzvq1CT2OHF094a-u_JnTNAOCp7iNYe1Pmw.png)

‍

-   **Feature use:** Which features are used and the most? Are your new
    features, or updates, getting used?
-   **Recency and frequency:** How recently did someone use your
    product? How frequently do they return?
-   **Value of use:** Are people who use a given feature more valuable,
    happy, or otherwise positively impacted by using it?

#### Website analytics

![High level web analytics data from Google
Analytics](https://global-uploads.webflow.com/59b1667dd2e65000019d07be/63926818bebe1cf14746c00d_Q3wwzTmXvn1kkTMhg3Wn9QopZoiPNaeGrs13eUI6JmDf4xFdQ_UkkRBNwdAQfux6lxA0rNVF6Jl7j_8StBFzhroxWTk528oYh5ErHQAJ7xAzUxx7N5boEjSzMgWo9n6sugjhjO-DtC-CeS-tFDjtGSj9Ssf9E-ogaH2dSl1jVW4GRlzov0o1Zmc51Y-n3A.png)

‍

-   **Time on site:** This can be a great area to drill into, especially
    for a content driven experience, or one where the revenue model is
    closely tied to visit duration.
-   **Visits:** How many people visited your experience overall? How are
    visits changing over time? More isn't always more, but all things
    being equal, it is.
-   **Unique visitors:** How many different users are interacting with
    the experience?
-   **Goal completion:** From leads generated, to purchases completed,
    to buttons clicked, if you can tag it, you can track it.
-   **Pages visited:** Which pages did an individual or group of
    individuals visit? Are those pages connected to key buyer/user
    journeys?
-   **Traffic source:** How did the user enter your experience? Was it
    from a certain campaign, organic traffic, or another source?
-   **Path to conversion:** Before converting, which pages do users
    visit? How long do they stay on site? Is this path as direct as
    possible? Is there an opportunity to improve your site navigation?
    Evaluate across different personas, lifecycle stages, or user
    stories. Not every user has the same goals.

‍

‍By understanding and tracking your baseline against the metrics that
matter to your goals and your users' goals, you'll quickly notice where
a product update or launch is having an impact.

‍

### 3. Set up a system for measuring analytics.

‍

Once you\'ve determined the metrics you need to track, audit the user
analytics systems teams are already using and look for any gaps in
capabilities. 

‍

Some user researchers build out custom dashboards, integrating Google
Analytics with Microsoft Excel, or taking advantage of one of the many
business intelligence tools now available. 

‍

Tracking data doesn't have to be a heavy lift. We recommend automating
as much as possible your dashboards to streamline your workflows. This
way, you can spend less time collecting data and more time analyzing and
acting on it. Make sure to share your dashboards with other stakeholders
too!

‍

#### A good dashboard:

-   Prominently highlights the most important data
-   Illustrates change, anomalies, data worth noticing
-   Is connected to deeper data for drilling where necessary to
    understand what's "really going on"

### 4. Set a cadence for reviewing analytics.

‍

Perhaps you have a weekly department meeting where key metrics review is
a recurring agenda item. 

‍

Or maybe you have a monthly OKR meeting. 

‍

Or maybe you are the lead researcher on a particular project you're very
invested in, and you need to know what's happening by the day or hour on
something that has just launched. 

‍

Depending on your individual, team, or company situation, set up a
regular cadence for reviewing the metrics that matter to you the most.
Of course, it may make sense to review different metrics at a different
cadence. 

‍

We recommend a weekly check-in of your top level key metrics at a
minimum, and then build from there based on your needs. Give yourself a
recurring task or calendar event to make this ongoing review a habit or
take advantage of automated email reports for your services that offer
them.

‍

### 5. Identify trends, user segments, and other patterns in the data.

‍

As you're reviewing your user analytics data, you may uncover patterns
and other points of interest that can inform, spark, and eliminate the
need for future qualitative studies.

‍

Be sure to:

-   **Look for points where users drop off.** These drop-off points
    often signal areas of confusion or other UX issues. Make note of
    them and explore them later with UX research. 
-   **Identify the most popular and least popular features.** The
    most-used features tend to be those that allow users to perform
    intended actions as simply as possible, while the least-used
    features tend to have poor usability or low value to users. Dig
    deeper into *why* users do or don't use these features in future
    studies. 
-   **Segment user personas.** Do different types of users interact with
    the product differently? If these groups are distinct enough, your
    team may benefit from using this data to inform user personas. 
-   **Outline user journeys.** What path (or paths) do users take as
    they're using the product? Can any steps be removed to make the
    process smoother for users? 
-   **Benchmark behavioral patterns to understand change.** As NN/g's
    Kate Moran talks about in [this Awkward Silences
    episode](https://www.userinterviews.com/blog/ux-benchmarking-to-demonstrate-roi-with-kate-moran-ux-specialist-at-nielsen-norman-group),
    analytics data can help you benchmark current usage and behavioral
    patterns to keep track of changes in those patterns over time. 
-   **Develop hypotheses regarding user motivations and product
    improvements.** Analytics data can only tell you what users did---it
    can't tell you why they did it. Using what you already know about
    customers, develop hypotheses about their motivations and evaluate
    the validity of these hypotheses with qualitative research. 

‍

### 6. Conduct formalized UX research to explore trends, questions, and opportunities.

‍

User analytics data will likely leave you with a list of unconfirmed
hypotheses about product improvements and unanswered questions about
user motivations. 

‍

Qualitative research comes in to answer those questions. AP Intego's Cat
Anderson explains this interplay between quantitative and qualitative
data on [Awkward
Silences](https://www.userinterviews.com/blog/ap-integos-cat-anderson-on-flowing-your-qual-and-quant-research):

‍

> "We use quantitative to inform our qualitative round. An example of
> that is when we draw from FullStory, the platform where you can record
> the screen as somebody\'s using your app or your website.... \
> \
> After watching hundreds of people struggle on this one interaction, we
> were then able to design a qualitative research round that really put
> it into a more holistic perspective and context for us: Why? Why is
> this a difficult step for you? Oh, well, it\'s because of all these
> other things that are happening around it in a small business owner\'s
> life or in a small business owner\'s journey. \
> \
> That was an example of... the quantitative informing the qualitative.
> And then, there\'s just a whole lot of complex interplay between those
> moving forward as we do new rounds of research." 

‍

User behavior analytics tools and software
------------------------------------------

‍

If you're already collecting user analytics data at your organization,
you might already be familiar with the most popular user behavior
analytics software, such as:

-   [Amplitude](https://amplitude.com/)
-   [Google Analytics](https://analytics.google.com/)
-   [Appcues](https://www.appcues.com/)
-   [Loop11](https://www.loop11.com/)
-   [Hotjar](https://www.hotjar.com/)
-   [Qualtrics](https://www.qualtrics.com/)
-   [UserPilot](http://com)

‍

🏰 ✨ To explore these and other user analytics tools in more depth, check
out the [2022 UX Research Tools
Map](https://www.userinterviews.com/ux-research-tools-map-2022), a
fantastical guide to the user research software landscape, with 230+
tools for recruiting, usability testing, surveys, moderated sessions,
analysis, and more.

‍

In a nutshell
-------------

‍

User analytics and UX research make a great team.

‍

By analyzing quantitative user analytics data---and pairing it with
qualitative data from UX research---you can identify areas for
improvement, create exceptional user experiences, and ultimately
increase retention and revenue. 

> *Note to the reader:*\
> \
> This part of the field guide comes from our 2019 version of the UX
> Research Field Guide. Updated content for this chapter is coming
> soon!\
> \
> Want to know when it\'s released?\
> [Subscribe to our newsletter!](#){.para-link-new .fg-coming-soon}


Once you've launched a new experience, you may feel as though you can
kick back, put up your feet, and wait for the positive feedback to roll
in. However, your work is far from over. In fact, in many ways, it's
just begun.

‍

Post-launch is a critical time to gain user feedback. That's because
your product/design is now in the wild, and real users are going to be
clicking buttons, navigating to new pages, and generally interacting
with everything you've created. This is yet another key time to listen
to your users---where are they having wins? Where are the stumbling
blocks? What works? What doesn't?

‍

Ongoing surveys are a great way to collect user feedback post product
launch. Here's how to get started from scratch, or more likely build on
systems you may already have running within your organization.

‍

Decide which survey methods you'll use
--------------------------------------

While there are countless survey tools and methods to choose from, the
good news is that you don't need a lot to set up a system that helps you
gain user feedback in a pretty automated way. Of course, you need to
make sure you're choosing a survey system that provides you with
feedback that you can act on.

‍

Here are some things to do as you assess survey methods:

‍

### 1. Align your efforts with business goals

When evaluating survey assessment options, start with knowing your goal.
Consider the overall goals of your product, business, and users, goals
that have probably been identified in prior research. Do you want to
improve customer satisfaction, increase conversions, or reduce customer
churn? Ongoing customer surveys can help you track metrics and
qualitative feedback related to how your users feel about the
experience. When it comes to metrics like churn, LTV, conversion rate,
we\'ll dive into those in our [**analytics
chapter**](https://www.userinterviews.com/ux-research-field-guide-chapter/user-analytics).

‍

### 2. Decide which ongoing assessment methods are right for you

Many organizations like Net Promoter Score (NPS) which divides your
users into promoters (those who would recommend the experience),
passives (those who are neutral), and detractors (those who have
substantial problems). Other options are Customer Satisfaction (CSAT)
and Customer Effort Score (CES).

‍

Here's an overview of some common assessments that user researchers use:

‍

#### Net Promoter Score (NPS)

Net Promoter Score segments users based on how likely they are to
recommend your product to a friend. It's a simple way of assessing
whether your experience can spread word of mouth, which counts for a
lot. Often users will also see an option to include why they scored a
particular way. A little quant, a little qual. Nice.

‍

![\
](https://global-uploads.webflow.com/59b1667dd2e65000019d07be/5ae7325551d4ae15fa60881b_Screen%20Shot%202018-04-30%20at%2011.11.39%20AM.png)

#### Customer Satisfaction Score (CSAT)

The Customer Satisfaction Score (CSAT) measures customer satisfaction by
asking users to rate their experience based on a predetermined scale.
CSAT is simple and easy to use, but since the question is so broad, the
reason behind the responses can be hard to decode. Still, in some ways
it is a more direct question than NPS and can help gauge overall
satisfaction in a more straightforward way.

‍

![A sample CSAT survey from
Hubspot](https://global-uploads.webflow.com/59b1667dd2e65000019d07be/61a69b9d467790c3e978f800_GoR7ffK-U_zsoxAQNiy_uQ9pcBlCuW1bz_DpT0stQ-5pclCTC_DtG6mjYPNueDDU7Qvqf_CBQ64iO6Wph51jRvPFyFEuyHUxLq8XSpLeUAQaCJnzlwNA96ccQIoEQRz2Lbwo1frP.png)

‍

#### Customer Effort Score (CES)

CES measures how much effort it takes for users to complete certain
tasks, such as contacting support to resolve an issue.

‍

![Nicereply shows what a CES survey looks like through its
platform](https://global-uploads.webflow.com/59b1667dd2e65000019d07be/5ae738c27359c4301d04e1ff_nicereply%20ces.png)

‍

#### Website Intercept Surveys

Website intercept surveys are essentially modals or similar that appear
at key points in the user journey to assess sentiment. This may seem
like an annoying addition to your site, but when implemented properly,
they can be relatively frictionless for your user, and provide key
ongoing feedback for you.

‍

Some common tools businesses use to support gathering these types of
surveys are Wootric, Promoter.io, GetFeedback, SurveyMonkey,
SurveyGizmo, Nicereply, and Zendesk.

‍

It\'s very possible someone or some team in your organization has
already established a tool or process of gathering one more of the types
of feedback you\'re interested in reviewing. You may be able to use what
they\'re already doing, or work with them to get the functionality
you\'d ideally want out of those tools.

‍

Make it easy for users to give feedback
---------------------------------------

As in many things user experience, when it comes to deploying an ongoing
user feedback survey, friction is your enemy. Here are some guidelines
to consider:

‍

### 1. Keep your surveys short and focused

Ask only a small handful of questions for your best response rate, and
to keep your analytical attention focused on the key goals you've set.
NPS, CSAT, and CES typically include 1-2 questions for a reason. If
you're building your own custom survey, make sure you're tracking a
consistent quantitative metric, while allowing for an open response that
gets at the "why" behind the what.  

‍

### 2. Pick a good time to solicit feedback

Is there an ideal moment in your user journey where it makes sense to
ask for feedback?

‍

For example, it makes sense for Google Maps to ask for a restaurant
rating shortly after you've added the restaurant as a destination in the
app or for Uber to prompt a driver rating right after a ride. Ask for
feedback while the experience is still fresh in the user's mind.

‍

Depending on your goals, you might ask at different times. If you\'re
focused on gaining adoption of a new feature, you might ask people who
are using it about how they found your app and their initial impressions
after using it. You might also proactively survey users to find out why
they haven't adopted it yet.

‍

Go back to your goals, and your status against any targets you've set
for launching a new product, then identify the users and events that
will give you the most insight to reach those goals. Finally, make sure
your surveys are deploying at the right time to achieve that.

‍

### 3. Consider where you make your ask

Next comes *where* to deploy your survey. Again, your options may be
limited to the services you're using, but here are a couple things to
consider when you decide where to deploy surveys.

‍

#### Keep it contextual

If there's a key moment to get feedback when someone is actively using
your web or mobile app, ask them directly in the app through a modal or
similar experience. If they miss prompt, you could follow up with an
email, judiciously.

‍

Email based support conversations are naturals for email feedback follow
ups, and in the same way a chat or messenger conversation can easily end
with a chat or messenger feedback request.

‍

#### Consider user preference

If you're sending proactive, versus user behavior triggered, surveys,
try to send them in the channels your users prefer. Email is a classic
choice here! But for your users who aren't subscribed to email, or those
who are more responsive via chat, in-app messaging, push, or other
channels, those may be more appropriate. Keep in mind push is likely
your most aggressive option, and you should watch your negative
KPIs---like opt-outs---closely when using it to request feedback.

‍

### 4. Ask for feedback early and often

The best way to do this is to take the key moments and channels you've
identified, then automate the delivery of the survey based on those
rules. Depending on the platform(s) you're using for your survey(s),
your ideal state may be seamless, or may take a little clever
Zapier-ing, but you should be able to get these kinds of ongoing surveys
to a largely autopilot state, freeing you to spend more time uncovering
insight and making better product and business decisions.

‍

And please make sure you're not berating your users. Look at your
ongoing survey program holistically, and take advantage of any frequency
capping or other options available to you through the platforms you\'re
using to make sure you are not overwhelming anyone.

‍

This seems simple enough, but in many organizations these surveys may be
owned by a variety of teams like support or marketing, so you'll need to
work with them to make sure your surveys are implemented in the best way
possible to get you valuable feedback, which is what everyone wants.

‍

In some cases, you may want to pair continuous feedback surveys with
other continuous methods like [continuous user
interviews](https://www.userinterviews.com/ux-research-field-guide-chapter/ongoing-customer-research)
for better insights.\

Build a system for implementing results
---------------------------------------

There's no point asking users to take a survey you can't analyze and
take action on. Ongoing survey data can help you find issues you didn't
know existed, separate signal from noise, understand the why behind user
behaviors, and more. Make sure you have a system in place to continually
analyze data and implement improvements. Here are our top
recommendations.

‍

### 1. Get the right people on board

Whether your research team acts as a service arm or your organization,
or works hand-in-hand with product to make decisions, it's essential
that you get the right people on board from the beginning. Work together
to assess the biggest priorities and determine how you will address
ongoing survey feedback in general.

‍

### 2. Decide on a schedule for analysis.

The thing about ongoing listening methods is that they're running all
the time, so when should you stop and analyze what's happening? If
you've rolled out a brand new experience for the first time, you should
be checking in very frequently, whatever that means for you. If you
haven't released anything huge recently, you might review on a less
regular cadence. Many services now integrate with email or Slack, so you
can stay on top of the day-to-day somewhat passively, while doing a
deeper dive on a more set, less frequent cadence.

‍

### 3. Implement changes based on what you've learned.

Make sure you have a way to bubble up the strong signals you're getting
from ongoing feedback. You may also identify quick wins in the form of
bugs or small usability issues that can make a big difference to the
user experience. In either case, building processes and relationships to
turn insight into action is critical to the success of your ongoing user
surveying initiatives. Make sure to document how the changes you\'ve
made based on ongoing survey feedback have improved the key metrics
you\'re tracking both regarding customer satisfaction, and broader
business goals (like retention or revenue).

> *Note to the reader:*\
> \
> This part of the field guide comes from our 2019 version of the UX
> Research Field Guide. Updated content for this chapter is coming
> soon!\
> \
> Want to know when it\'s released?\
> [Subscribe to our newsletter!](#){.para-link-new .fg-coming-soon}


Your support and sales teams are on the front lines, interacting with
users on a day-to-day, hour-by-hour basis. Because of this, your
customer facing teams are a great ongoing source of qualitative user
insight. Developing ways to access their knowledge, build relationships
with them, and align your interests are important aspects of managing
your ongoing listening, post-launch research.

‍

Build connections with sales and support
----------------------------------------

We talked a bit about ongoing surveys, like NPS, CSAT, and CES, in a
[**previous
chapter**](https://www.userinterviews.com/ux-research-field-guide-chapter/continuous-user-feedback-surveys).
You may already be working in conjunction with your support and sales
teams to collect and analyze that data. Great, you have a leg up. This
data is great because once you have the processes set up, it keeps
coming to you in a pretty automated way, and you have a single metric or
two you can track over time to baseline against. For those reasons,
quantitative data is really useful for ongoing listening, but
[**qualitative data plays an important
role**](https://www.userinterviews.com/blog/how-to-analyze-qualitative-data-with-the-design-gym)
too in understanding why users are reacting positively or negatively to
your product.

‍

Much of the data you can get from your sales and support teams will be
qualitative, based on the stories and feedback they hear each day. A
great way to encourage the exchange of this qualitative data is to build
relationships with customer facing teams. This will facilitate the
organic passage of knowledge, as well as help lay the foundation for
more systematic processes. If you're at a large organization, start by
identifying who has the information you need. Then, make friends with
them! Figure out how your work can benefit each other, and start coming
up with ways to make those things happen.

‍

Implement a system for collecting and organizing feedback
---------------------------------------------------------

One of the biggest stumbling blocks when it comes to collecting
qualitative feedback is making sure there's a system of record for
organizing it all. If someone on your sales team tells you that everyone
struggles with a certain feature, how are you going to document that?
How will everyone access it when it\'s most relevant and actionable?

‍

You can do this via a spreadsheet, project management tool, and/or via
regular meetings where someone takes notes, and adds the insights to the
company wiki say, to name a few examples. Recently, many organizations
are using [**Airtable to capture
\"nuggets,\"**](https://medium.com/@WeWorkUX/the-atomic-unit-of-research-insight-17d619583ba)
or small units of user insight that can be accessed on demand, when
building a product that addresses that situation for instance. We
actually use this solution at User Interviews!

‍

![Nuggets in
Airtable](https://global-uploads.webflow.com/59b1667dd2e65000019d07be/5ae766b241c712ebdbdddece_Screen%20Shot%202018-04-30%20at%202.55.18%20PM.png)

‍

Another recommendation is to break up the kinds of feedback that are
useful to various teams in the company (sales, support, marketing, etc).
Different teams can filter based on which team submitted the feedback,
or which team it is most relevant to. The point is, often user feedback
is relevant to many teams, so keeping that feedback together in one
place for all to see makes a ton of sense, and marks meaningful progress
for many organizations toward democratizing research and the user
experience in general. Make sure to discuss this data on a regular
cadence, so it doesn't fall into the ether.

‍

Analyzing support and sales experiences
---------------------------------------

An essential aspect of UX research, and[ongoing
listening](https://www.userinterviews.com/ux-research-field-guide-chapter/ongoing-customer-research)
in particular, is analyzing user interactions with your support and
sales teams across your website, email, chat, demos, or call center.
Work with your sales and support leadership to find ways to evaluate
this data. Perhaps sales or support puts together a weekly or monthly
report on what they\'re hearing on the frontlines, with an emphasis on
trends. Or, maybe you set up a recurring one-on-one with the same, of
different members of these teams to accomplish a similar goal. Perhaps
representatives from each team come together for a \"voice of the
customer\" session.

‍

If your system for organizing feedback includes the key kernels of
insight gathered across email, chat, and phone, that may be enough for
your needs. But take the time to understand where the data is, what kind
of data is available, and make sure you have access to what you need to
stay close to the user after a product has launched.

‍

### Go directly to the knowledge base

Your company likely has a self-service knowledge base where users can go
to ask questions and get the answers they're looking for. Using your web
analytics data, you'll be able to understand which articles get the
most/least views and which ones are marked as the most/least helpful.
You can look at what people search for most frequently, where they
click. This data can give you insights into where improvements can be
made in your product. If people have a lot of questions about a
particular topic, is it because the product experience could be more
seamless in those areas?

‍

Because a knowledge base is sort of an extension of the product and
support team, you may also uncover opportunities to improve the
knowledge base in this process. Creating content for searches that
return no results, or even running further qualitative usability testing
to make sure people can find and understand the content they need. Work
with your support and product teams to tackle the best opportunities on
a regular cadence here.

‍

On bugs
-------

Bugs stink. But when you're building new products, and constantly
iterating on them, bugs happen. Some bugs are relatively
harmless---they're mildly irritating but don't ultimately prevent users
from accomplishing key tasks. But there are also bugs that can cause
major interruptions, driving users away.

‍

Before releasing a new experience, you do everything you can to find and
squash bugs. Often, QA teams get involved, as well as other software
developers. Even so, bugs crop up. As a UX researcher, bug reports can
be useful in your ongoing listening methods toolkit.

‍

Take it from [**Itamar
Turner-Trauring,**](https://www.userinterviews.com/ux-research-field-guide-chapter/integrating-support-sales-and-product-feedback#)
Founder of [**Code Without
Rules:**](https://www.userinterviews.com/ux-research-field-guide-chapter/integrating-support-sales-and-product-feedback#)

> Your software has bugs. Sorry, mine does too. Doesn't matter how much
> you've tested it or how much QA has tested it, some bugs will get
> through. And unless you're NASA, you probably can't afford to test
> your software enough anyway. That means your users will be finding
> bugs for you. They will discover that your site doesn't work on IE
> 8.2. They clicked a button and a blank screen came up. Where is that
> feature you promised? WHY IS THIS NOT WORKING?!

‍

Thankfully, as a user researcher, you can make use of what your users
are telling you.

‍

### Why getting bug feedback from users matters

If a user interacts with your experience and consistently finds bugs,
this is obviously a problem. The user will not feel positively about
your brand, even if they are still able to complete the tasks in
question. According to Turner-Trauring, users have only [**two options
when they confront
bugs.**](https://www.userinterviews.com/ux-research-field-guide-chapter/integrating-support-sales-and-product-feedback#)They
can:

‍

![](https://global-uploads.webflow.com/59b1667dd2e65000019d07be/5ae74a47ee38f17b89b74bb8_Screen%20Shot%202018-04-30%20at%2012.53.50%20PM.png)

‍

If you constantly fix your bugs, users are likely to be happier. They'll
stick around. However, it's not fixed bugs that make users loyal to an
app or experience. What's most important is that these users feel heard.

‍

### Implementing a bug feedback system for users

A good bug feedback system means users are able to report bugs as
quickly and as easily as possible, and then feel that your team hears
their feedback, responds to it, and follows up.

‍

Ask yourself:

-   How do users currently report bugs?
-   How often are users reporting bugs?
-   When users report bugs, do they get a customized reply, an
    auto-generated reply, or nothing at all?
-   Are users ever notified when the bugs they report are fixed?

‍

### Tips for reviewing bugs

Do you have an internal Slack channel, dedicated email, or software that
helps you identify and share news of bugs? Probably your product or
engineering team has some systems in place you can take advantage of as
a source of data for your ongoing listening efforts.  

‍

FAQ and support desk reporting can similarly help you understand where
users are getting caught up or confused in the product experience,
reflecting usability issues that might not actually be bugs---someone
internally may have even thought they were features!

‍

Users don\'t always know if they're encountering a bug or a usability
issue. Your \"bug reporting\" system therefore likely straddles several
of the above types of reports. Work with your support and engineering
teams to find the right sources of data to uncover key bugs and
usability issues, even if everything isn't always clearly delineated in
exactly the right place.

‍

### What to do with bug reporting

We'll make the assumption that someone on your product or engineering
team is actively working to improve bugs, but this reporting has other
value too beyond fixing what's broken. If certain tasks or areas of your
product have had a history of bugginess, that's good insight to be aware
of when you're evaluating how to improve that experience. It may be the
at the experience itself is pretty good, it was just riddled with bugs
in the past. If NPS drops in a given week, it may be connected to a
string of bugs, and not that you built the wrong feature the wrong way
on a broader level.

‍

The best way to use bug reporting for post launch ongoing user research
is to keep an eye on the volume of bugs coming in comparatively over
time, and the nature of those bugs. This will add a layer of useful
context to your analysis of a product\'s rollout and success as you look
to make constant improvements.

‍

Putting it All Together
-----------------------

Sales, support, product, and perhaps even marketing, have user data you
can use. You probably have data they can use too. Understand who has
what, where and how they\'re collecting it, and seek to keep your
insights accessible, organized, and actionable. The more all departments
share a common language and insights about user feedback, the more your
organization can march forward on a cohesive mission to improve the user
experience across every touch point. There are infinite ways to get
there, but the underlying principles are the same.

‍

Aristotle said, 

> "We are what we repeatedly do. Excellence, then, is not an act but a
> habit."

Likewise, excellence in UX research is not achieved with a single study,
but with *habitual* acts of research. Of course, one-off studies are
still incredibly important, and we're not suggesting you drop them in
favor of continuous research. Instead, combining one-off studies with
continuous research can take your UXR practice to a whole new level. 

‍

When you [make it a habit to interview
customers](https://www.userinterviews.com/blog/4-tips-to-make-ux-research-a-regular-habit)
on a weekly, biweekly, or monthly basis---whatever cadence works best
for you---you can build your research muscle and use fresh, relevant
customer insights to drive your decisions.

### In this chapter:

-   What are continuous user interviews?
-   Benefits and challenges of continuously interviewing
-   Why does continuous research matter?
-   How to [build a participant panel for continuous
    research](https://www.userinterviews.com/blog/build-manage-research-participant-panel)
-   Tips for conducting continuous interviews, from planning to sharing
    insights
-   Helpful tools for continuous user interviews
-   Mixed methods for continuous research

What are continuous user interviews?
------------------------------------

**Continuous user interviews** are frequent 1-1 meetings with customers
for the purpose of collecting ongoing insights. Unlike [user
interviews](https://www.userinterviews.com/ux-research-field-guide-chapter/user-interviews)
conducted as part of a dedicated study, continuous user interviews are
quicker, recurrent, and open-ended in nature. They're an important part
of the [continuous UX research
framework](https://www.producttalk.org/2021/05/continuous-discovery-habits/)
and, more broadly, the [continuous delivery
framework](https://www.userinterviews.com/blog/the-7-step-framework-pluralsight-uses-to-ship-customer-centric-product-continuously-that-you-can-steal-too)
used by agile product teams.

‍

Specifically, continuous user interviews fall into the 'continuous
exploration' stage of the continuous delivery process, as demonstrated
in [this graphic by Scaled Agile,
Inc](https://www.scaledagileframework.com/continuous-exploration/): 

![](https://global-uploads.webflow.com/59b1667dd2e65000019d07be/6331e12cc08f95b9550876a2_MAgFqfoc562KrmLnKAqNSdQMuerd_USFAurPtT8ccqw1-_hEvlqGGhM8cSBAO6ntkmmF-XY4FJCwRPSMiGH0xP9NQPiWHQUNKIKs8eyRbwLDIA2BImzqr5n9Nv9kSYwsvIkHBuam9s8nu_6om-fL4tiqp3BedApQdzPVdlVcMvOmFUA89XS3-LX0AQ.png)

‍

[Product Discovery Coach Teresa
Torres](https://www.youtube.com/watch?v=_KkDXQw7kqg) describes
continuous research as like "putting money in a bank"---in other words,
recurring interviews allow you to collect insights, like coins, which
can have a compounding effect over time. 

‍

#### Types of continuous interview programs

Continuous interviews can take many different forms, including:

-   **Broad customer feedback interviews**, done on a weekly basis and
    focused on general discovery. 
-   **Team-based** [**customer
    interviews**](https://www.userinterviews.com/blog/the-ultimate-guide-to-doing-kickass-customer-interviews),
    done on a weekly basis by individual teams, focused on themes
    specific to that team.
-   **Structured user interviews**, occurring during specific stages of
    the design and development process and focused on answering targeted
    questions. 
-   **Recurring advisory board interviews**, occurring on a regular
    basis to build long-term relationships with customers and streamline
    future recruiting efforts. 

‍

When you're deciding what continuous interviews should look like at your
company, focus on your goals. Every organization is unique when it comes
to how their team is structured and what they\'re hoping to accomplish
with continuous discovery, so it may or may not be the right approach
for you.

‍

For example, if you'd like to generally create more opportunities for
your team to interface with customers and boost empathy across the org,
you might lean toward broad or team-based interviews. If you'd like to
simplify recruiting for one-off studies, you can build an advisory board
to have the infrastructure in place when you need to access a pool of
participants. 

‍

Or, you might choose to implement some combination of the different
types to encourage diverse and recurring conversations with customers
throughout the organization (in addition to project-based studies for
deeper analysis). 

### Benefits of continuously interviewing your customers

If it weren't obvious from our name, we're big fans of user interviews.
But adding *continuous* user interviews into the mix? Even better. 

‍

Here are some of the benefits and opportunities associated with
continuous interviews.

‍

#### 1. Seize opportunities when they're ripe (or pivot before too much damage is done). 

‍

Continuous user interviews allow you to discover opportunities, pain
points, risks, and other customer feedback that can lead to new ideas or
product iterations (or [stop a bad product
decision](https://www.userinterviews.com/blog/design-failure-examples-caused-by-bias-noninclusive-ux-research)
in its tracks).

‍

As Laura Carroll, Sr. Design Manager at Medium, says in [her article on
implementing continuous user
research](https://medium.design/implementing-continuous-user-research-at-medium-e32825641d9b): 

‍

> "\[Continuous research---as opposed to a traditional study
> approach---\] creates a deeper understanding of users by creating more
> space for open dialogue than a study focused on a specific feature or
> problem area would."

‍

#### 2. Prioritize next steps on your product roadmap.

‍

Most product teams have a long list of "wishlist" items that they'd like
to work on (exhibit A: [User Interviews\'s product
roadmap](https://www.userinterviews.com/roadmap)). 

‍

The trouble is, it can be difficult to know for certain which projects
are worth working on, and when to tackle them. When you're regularly
checking in with customers, you'll likely hear them bring up specific
goals or pain points that help you decide what to work on next. 

‍

Product Discover Coach and Founder of [Product
Talk](https://www.producttalk.org/) Teresa Torres describes this benefit
in [her blog on continuous
interviewing](https://medium.com/@ttorres/continuous-interviewing-the-key-to-successful-product-teams-6bf63bfc1936): 

‍

> "When a product team develops a weekly habit of customer interviews,
> they don't just get the benefit of interviewing more often, they also
> start rapid prototyping and experimenting more often. They do a better
> job of connecting what they are learning from their research
> activities with the product decisions they are making."

‍

#### 3. Create a research-first culture on your team. 

‍

Last but not least, continuous interviewing helps foster an appreciation
for---and habitual use of---research throughout your organization. 

‍

With regular user check-ins, you can create a healthy culture of
curiosity and openness among your team. Research becomes a need-to-have
practice of its own, independent of [confirmation
bias](https://www.userinterviews.com/blog/on-psychology-and-user-research-with-lorie-whitaker-of-rackspace)
or the urge to just "check boxes." 

‍

Gregg Bernstein, User Research Lead at Condé Nast, joined [the Awkward
Silences podcast to talk about healthy research
cultures](https://www.userinterviews.com/blog/healthy-research-culture-gregg-bernstein-conde-nast):

‍

> "A sign of an unhealthy research practice is where research is
> pigeonholed into: 'we\'re only going to use research to test or
> validate something that\'s already been decided.'"

‍

In other words, continuous research broadens your teams'
conceptualization of research, from a step that's only taken when
specific questions arise to **a process that can spark questions no one
would've thought to ask otherwise. **

‍

### Challenges and risks to be aware of when developing a continuous interviews habit

‍

You might be thinking, "the more research, the better, right?" 

‍

Not necessarily. As you conduct user interviews on a more frequent
basis, you may run into some growing pains. 

‍

Although we're a big proponent of continuous user interviews, we also
understand that they come with some challenges and risks. These
challenges can be mitigated with careful planning and effective tools,
but it's important to be aware of (and plan for) them ahead of time. 

‍

The main drawbacks of continuous user interviews are: 

1.  **They take time---**but, you can minimize the time you spend on
    continuous user interviews by keeping them short and sweet, pairing
    them with unmoderated methods like in-app surveys, and allowing
    [PwDRs (people who do research outside of the core UXR team, like
    designers or
    marketers)](https://www.userinterviews.com/blog/people-who-do-research-discovery-study)
    to conduct them on their own. **‍**
2.  **They could cause participant fatigue**---but, you can minimize
    this risk with a panel management solution like [Research
    Hub](https://www.userinterviews.com/research-hub) to track
    participation, set contact limits, and create recruitment guardrails
    for your team. You can also pair customer recruitment with
    recruitment from an external panel to supplement your research
    without over-contacting customers.

More broadly, why does continuous research matter?
--------------------------------------------------

‍

Continuous user research **reduces the risk** involved in product
development by grounding all of your decisions in tangible user
insights. 

‍

![[Graph by Product Discovery
Methods](https://pdmethods.com/user-research/how-to-conduct-user-interviews/)](https://global-uploads.webflow.com/59b1667dd2e65000019d07be/6331f70f7c4b025117274fbc_9NfZ9D3WbFnmm7tVjCX3nKE4mTqp5UgEohmCT57fM72r0dnoQZXFHaxAiF8ET1pijDXRlWwjLj7AYycMpC5FfftmDx_NIi4eNZ8SX0_pIrJ8lYIjBYja8kk3CgflERwWFQqPAcUaLr_p1-Uo1l3GvrLSGP829pn1WQid5zVSRrI-eFzeQNFEHCk1Sw.png)

‍

When Teresa Torres, product discovery coach and founder of Product Talk,
[joined the Awkward Silences
podcast](https://www.userinterviews.com/blog/how-to-interview-customers-continuously-with-teresa-torres-of-product-talk),
she discussed the history of user research and how it's evolved to
include continuous research: 

‍

> "We\'ve seen over the last 15, 20 years, a lot of growth in user
> experience design and user research. But a lot of that growth has been
> fueled by a project mindset... so there\'s this idea that research is
> a phase that happens at the beginning....\
> \
> I think what we\'re finding as we move towards more of a continuous
> improvement mindset, both on the delivery and the discovery side, is
> that there\'s always questions that could benefit from customer
> feedback."

‍

In other words, research has shifted from one-off projects that answer
[specific research
questions](https://www.userinterviews.com/ux-research-field-guide-chapter/user-research-questions)
to an ongoing practice that reminds us that users will always be the
best advocates for themselves. Teams are often surprised, disappointed,
or taken off guard by the insights they learn from routine interviews,
and these insights lead products and businesses in unexpected (yet
valuable) directions. 

‍

Oyster's Lead UX Researcher Dr. Maria Panagiotidi says in [her how-to
guide to introducing continuous research
habits](https://uxpsychology.substack.com/p/from-no-research-to-continuous-research): 

‍

> "Continuous research is a form of proactive research---we constantly
> schedule user interviews without having a specific project. When we
> don't have a focused project, the sessions can be used to help us
> discover user pain points and uncover opportunity areas. In cases we
> have specific questions, continuous research allows us to speed up
> recruitment!"

‍

Alas, continuous research and project-based research **inform** and
**accelerate** each other. 

‍

Speaking of recruitment....

How to build a participant panel for continuous user interviews
---------------------------------------------------------------

‍

[Recruitment](https://www.userinterviews.com/ux-research-field-guide-module/recruiting)
is consistently---across projects, teams, and industries---one of the
biggest hurdles researchers face. 

‍

As Teresa Torres says on [the Awkward Silences
podcast](https://www.userinterviews.com/blog/how-to-interview-customers-continuously-with-teresa-torres-of-product-talk): 

‍

> "I think the biggest \[objection\] I hear is, I\'m not allowed to talk
> to customers. I don\'t know how to find customers. Takes three weeks
> to recruit customers. I really think the biggest barrier to continuous
> interviewing is getting somebody to talk to on a regular basis."

‍

So how do you manage this challenge? Teresa says the very first thing to
do is **automate your recruiting process**. 

‍

📚 If recruitment is a major bottleneck for you, we've created [an entire
guide on how to break this bottleneck, speed up research cycles, and
improve the impact of
UXR](https://www.userinterviews.com/blog/how-to-break-open-the-recruiting-bottleneck).
We recommend downloading a copy of the guide to have on hand for
recruitment best practices, but here are a few tips for recruiting and
managing an internal participant panel in the meantime:

‍

1.  Map out your technical requirements.
2.  Assign panel ownership, roles, and responsibilities.
3.  Define participant criteria.
4.  Develop an incentive plan.
5.  Reach out!

### 1. Map out your technical requirements.

‍

Before you can start reaching out to customers to sign up for your
panel, you need to make sure that all of your technical boxes are
checked. 

‍

To effectively [build and manage an internal participant
panel](https://www.userinterviews.com/blog/build-manage-research-participant-panel),
you'll need:

-   A sign-up form and [screener
    survey](https://www.userinterviews.com/ux-research-field-guide-chapter/screener-surveys)
    with professional branding
-   Some way to deliver the sign up form to customers (e.g. a dedicated
    webpage or social link)
-   A place to securely store, update, manage, and (when requested)
    delete participant data
-   The ability to filter, segment, and export participant lists based
    on specific criteria
-   An email for communicating with participants (we suggest creating an
    official research@ email address for an extra dose of
    professionalism and credibility)
-   The ability to easily schedule session times with participants
-   [Incentives](https://www.userinterviews.com/blog/the-ultimate-guide-to-user-research-incentives)
    redeemable wherever participants are located
-   A request/ticketing system for any mishaps that might occur

‍

... as well as any other technical requirements specific to your
company, such as a means for collecting signatures for
[NDAs](https://www.userinterviews.com/blog/ndas-and-informed-consent-for-user-research). 

‍

### 2. Assign panel ownership, roles, and responsibilities. 

‍

Who's in charge of this thing?

‍

Even if multiple teams across your company intend to recruit
participants from your internal panel, it still helps to assign a
dedicated panel manager for oversight and quality control. 

‍

As Jeanette Fuccella, Director of Research & Insights at Pendo.io and
former Principal User Experience Researcher at LexisNexis, says in [her
article about managing an internal user research
panel](https://medium.com/lexisnexis-design/managing-a-user-research-panel-8b717ebeda47): 

‍

> "By all measures the panel has been a huge success, but we would never
> have been able to achieve this success without one key critical
> component: our panel manager."

‍

Typically, [panel
management](https://www.userinterviews.com/blog/ethnio-alternative-user-interviews)
would fall under the umbrella of [research
operations](https://www.userinterviews.com/blog/research-ops-what-it-is-and-why-its-so-important),
and include responsibilities like:

-   Recruitment and
    [screening](https://www.userinterviews.com/blog/screening-participants-in-research)
-   Communication with participants, including session reminders and
    fielding any questions they may 
-   Updating panel records with changes in personal information and
    participation status
-   Controlling panel access and maintaining data security 

‍

... in addition to the other core functions of [a Research Ops
practice:](https://www.userinterviews.com/blog/kate-towsey-on-starting-a-researchops-practice) 

​​

![[Infographic by Nielsen Norman
Group](https://www.nngroup.com/articles/research-ops-101/)](https://global-uploads.webflow.com/59b1667dd2e65000019d07be/6331f7106042925247f38bf6_vwaPfSsi01xYFx1rLZ8l8sHrj5MvsRBycymCcAmYP7eg-6-6A1MJj9wM5MNz1A14dYJm4nqMamoRwhpvxzcM4vHvdcndPXeKBH7foPDQi2QJ0F77fAiH26Wmz8lkq9VKCgUujvtCjm0coh8N93S4oN-gzU0mIRYgjdctpjYm9Wz1SabPPsWELcfFRw.png)

‍\
However, if your organization doesn't currently have a [dedicated ReOps
function](https://www.userinterviews.com/blog/research-operations-supports-scaling-democratizing-ux-research),
you can assign one of your user researchers as the panel owner. Just be
mindful of how many teams and projects they're supporting at any given
time---recruitment and panel management are big jobs, so it's not
reasonable to expect a lone researcher to handle them for large, scaling
teams. 

### 3. Define participant criteria. 

‍

As with any study, you'll want to be intentional about the type of
participants you recruit to be on your panel. To get the most accurate
insights, the users on your panel should be a diverse and varied group
of people, reflective of your actual user base. 

Consider criteria like:

-   How long the participant has been a customer,
-   Which product or pricing plan they're using
-   Whether or not the customer is high-value and engaged or a more
    casual user of the product
-   Whether or not the customer is predisposed to having a
    better-than-average opinion of your product or business (e.g. your
    parents may be 'customers', but probably aren't in the best position
    to give you unbiased insights)
-   The number of studies the customer has already participated in, and
    whether or not past participation might jeopardize the quality or
    accuracy of the interview

‍

Additionally, customers who actively sign up to participate on a user
panel might have different characteristics---for example, greater
extroversion, stronger opinions, or more engagement with your
product---than those who decline your invite.

To get the best results, be sure to balance insights from your panel
interviews with other types of research, such as [A/B
testing](https://www.userinterviews.com/ux-research-field-guide-chapter/a-b-testing)
or NPS surveys. 

‍

### 4. Develop an incentive plan. 

‍

Incentives are a great way to encourage panel sign-ups and thank
participants for their time. 

‍

But before you start doling out cash, you should think about:

-   **What type of incentives should you offer?** Common incentive types
    include monetary rewards, product discounts, charitable donations,
    or swag. Because continuous interviews happen over a long period of
    time, it might not be feasible to offer monetary incentives every
    time. Do the math to figure out what will work for you long-term. 
-   **What is the right incentive amount?** The right amount of
    incentive is 1) a big enough value to be worth the participant's
    time and 2) a low enough value to be affordable and sustainable for
    your company. Check out our [UX Research Incentive
    Calculator](https://www.userinterviews.com/lp/ux-research-incentive-calculator)
    for help finding the best incentive that you can reasonably afford. 
-   **When should you distribute incentives?** Most researchers only
    offer incentives after confirmed participation in a study, but it
    might be worth offering incentives just for signing up to your panel
    in the first place (especially in the early stages, or if you're
    struggling to [recruit good
    panelists](https://www.userinterviews.com/ux-research-field-guide-chapter/find-good-research-participants)). 
-   **How should you distribute incentives?** You can distribute
    incentives manually using email or gift cards, or you can automate
    incentive distribution using tools like
    [Tremendous](https://www.tremendous.com/). If you use [User
    Interviews](https://www.userinterviews.com/support/distributing-incentives)
    for recruiting and panel management, we can take care of incentives
    for you. 

‍

📚 Incentives are a big topic, and we could talk about them all day---but
this isn't the place. [Head to the UX Research Incentives chapter to
learn
more.](https://www.userinterviews.com/ux-research-field-guide-chapter/ux-research-incentives)

‍

### 5. Reach out! 

‍

Once you have the logistics, criteria, and incentives plan nailed down,
you can start recruiting customers. 

![[Example of an email invitation using Research
Hub](https://www.userinterviews.com/support/research-hub-participant-experience)](https://global-uploads.webflow.com/59b1667dd2e65000019d07be/6331f710218dff39e349b2dc_pFPeMCR-XheuFqFOAxpVjIW5CBV0iNTlCIKeXRGmOkrjt7LmBec0io4YlWC1ru-Nwq6NCmuzZPFSCvMdZWngoZFwEA8rfCME8FYk3iWReTFoFXhBIx8XpHS3pw9uwdoUcs-qzgvq4aQ6jny66RS9UtpOU0qVWkPZ4NIQWAF_3uSMcYZMWTGuLlEAdg.png)

‍

Some options for soliciting panel sign-ups include:

-   **Opt-in website forms** on a static webpage or using a
    pop-up/chatbot tool like [Drift](https://www.drift.com/).
-   **Emails** sent via tools with automated communication features like
    [User Interviews](https://www.userinterviews.com/research-hub) or
    [HubSpot](https://www.hubspot.com/?__hstc=182872057.99a9b12a9292e50672493f664e4e8a5e.1674490555104.1674490555104.1674490555104.1&__hssc=182872057.7.1674490555104&__hsfp=4039929895).
    You can create templated [recruitment
    emails](https://www.userinterviews.com/blog/recruiting-user-research-participants-by-email)
    and ask account managers to send them for you; this is a
    less-efficient solution, but common in enterprise companies where
    there's more cross-team coordination involved. 
-   **In-app surveys** using tools like [Sprig](https://sprig.com/) or
    [Appcues](https://www.appcues.com/).\*
-   **Direct links** to panel sign-up forms via social media or
    platforms like Craigslist.
    [SurveyMonkey](https://www.surveymonkey.com/), for example, allows
    you to collect responses with a direct link in social posts. 

‍

\*💡 [Pro
Tip](https://www.userinterviews.com/blog/a-framework-for-continuous-research-sprig-webinar)
from Allison Dickin, Sprig's Staff User Researcher, and Paolo Appley,
User Interviews's own Senior Product Manager: In-app surveys shouldn't
interrupt an important product workflow---otherwise you risk frustrating
users. 
:::


-----------------------------------------

‍

Okay, so you've built your panel. Now what?

‍

Here's how to [design and conduct user
interviews](https://www.userinterviews.com/blog/the-ultimate-guide-to-doing-kickass-customer-interviews)
as a habitual practice in the continuous research framework: 

‍

1.  Set a length and cadence that works for you (and your users).
2.  Decide which teams should be a part of the conversation.
3.  Prepare a (brief) interview guide.
4.  Make it as easy as possible for participants to schedule (and show
    up to) sessions.
5.  Be an open, active, and empathetic moderator.
6.  Follow up with participants.

‍

Let's dive deeper into each step. 

‍

### 1. Set a length and cadence that works for you (and your users).

‍

One of the most common objections to starting a continuous user
interview practice (and, really, one of the most common excuses for not
doing *anything* in life) is: "I don't have time."

‍

Here's why that excuse is bunk: Continuous interviews don't have to be
very long. 

‍

#### How long should continuous user interviews be?

‍

As we recommend in our [Continuous User Interviews Launch
Kit](https://www.userinterviews.com/launch-kit/continuous-user-interview),
one 30-minute session per week is ideal---but even just 5 minute
check-ins every week can work, if that's all you can manage. 

‍

Continuous interviews are all about building the habit, so choose a
length of time you can realistically keep up with.

‍

#### How often should you conduct continuous user interviews?

‍

As far as the cadence of interviews goes, [Teresa Torres, product
discovery coach and founder of Product Talk, recommends once a
week](https://www.userinterviews.com/blog/how-to-interview-customers-continuously-with-teresa-torres-of-product-talk). 

‍

If you're not there yet, challenge yourself to reduce the cycle time
between interviews throughout the year. For example, if you're doing
1/quarter, try for 1/month, then if you're doing 1/month, try for
1/week, and so on. 

‍

💡 **Pro tip:** Focus on the frequency of interviews, not the number of
people you talk to. As [Teresa says in the Awkward Silences
episode](https://www.userinterviews.com/blog/how-to-interview-customers-continuously-with-teresa-torres-of-product-talk):

‍

> "I think the number of people you talk to is the wrong metric.... If
> you have a glaring usability problem, and the first person you
> interview helps identify that, well, you don\'t really need to talk to
> another person.... Other things are more complex, and you might need
> to talk to more people. But I think this metric of reducing the cycle
> time between customer touchpoints encourages the right behavior of:
> 'How do we just talk to customers as frequently as possible?'"

‍

#### When should you conduct continuous user interviews? 

‍

Whenever it works for you (and the participant). We recommend blocking
off an hour or so each week to dedicate to interview sessions. 

‍

Outside of these regular interviews, you can schedule interviews at key
milestones in the customer lifecycle to learn more about the specific
experience of those milestones. Other opportunities for regular user
interviews include:

-   As part of the [onboarding
    process](https://www.userinterviews.com/blog/onboarding-ux-how-to-research-and-design-a-great-first-impression-pulkit-agrawal-chameleon)
-   When a customer begins using a new feature
-   After customers churn

### 2. Decide which teams should be a part of the conversation.

‍

In general, it's better to involve multiple teams---not just the UX
research team---in the interview process, for two reasons: 

‍

1.  **To capture as much insight as possible:** A product manager,
    designer, and engineer could all listen to the same interview, and,
    because of their different perspectives and expertise, will hear and
    remember different information. With more than one perspective
    represented, you can extract more value from each interview.
2.  **To reduce time and cognitive load on individual moderators:**
    Conducting sessions is a time-consuming (and sometimes
    stress-inducing) process. When you share this work, you enable each
    moderator the time and mental space to conduct [better, more
    effective
    interviews](https://www.userinterviews.com/blog/leveling-up-user-interviews-therese-fessenden-nng). 

‍

Of course, you don't want to invite a dozen people to each
interview---how overwhelming that would be for the participant! The key
is to strike a balance between strategically sharing interviews among
teams and allowing each team to conduct interviews independently.

‍

[GitLab, for example, uses a continuous interview
framework](https://about.gitlab.com/handbook/product/product-processes/continuous-interviewing/)
in which product managers lead interviews, while allowing other team
members to jump in to observe:

‍

> "Continuous interviews are open to all GitLab team members. The PM
> should notify their team Slack channel about upcoming interviews. All
> the team members and stable counterparts within a group are encouraged
> to take part in the interviews from time to time, so they can have
> first-hand experience listening to customer problems."

‍

### 3. Prepare a (brief) interview guide.

‍

You're probably familiar with [interview
guides](https://www.userinterviews.com/launch-kit/user-interview) for
project-based studies. The interview guide for a continuous practice can
be more concise, general, and open-ended. 

‍

#### "Tell me about a time when..."

‍

In fact, [Teresa Torres
says](https://www.userinterviews.com/blog/how-to-interview-customers-continuously-with-teresa-torres-of-product-talk)
that just *one* question can do the trick for continuous interviews:

‍

> "You might have some warm up questions, just to build rapport. But the
> meat of your interview is what I think can be driven by one question.
> And it\'s a 'tell me about a time when' question."

‍

Why does "tell me about a time when..." work? Because it elicits a
**story** from the participant, and stories are more likely to give you
insight into the participant's decision-making process and mental
models. 

‍

#### Follow-up interview questions

‍

However, it's probably a good idea to plan out additional questions in
case the first question doesn't elicit the detailed response you hoped
for. Here's an example of how two different participants might answer
the same question, and how to probe for more information when
necessary: 

‍

![[Example of probing questions by
NN/g](https://www.nngroup.com/articles/user-interviews/)](https://global-uploads.webflow.com/59b1667dd2e65000019d07be/6331f81270baaaffa06b65f7_r68hyi5mMluEycVO1nAESIJScIynhBRpRqA-nBTkRFcdMmS1V38ntKMpsfpcyjOzBIUb8-qw81qHqS5VY6Yvb8WhSce5ilDnvWZyQ7MdOvNajeS71K8LuXhh83HpWQTVp_9ncOuglwH9slg8zkZL_Qp-F8UhfWEG48BqODKA3P1OQ8GDAfib2GtJsQ.png)

‍

#### Customers aren't clairvoyant 

‍

Additionally, it's helpful to ask questions about the past, not the
future. This tip comes from Digital Product & Technology Transformation
Leader Michaela Heigl in [her article on continuous
interviewing](https://www.linkedin.com/pulse/continuous-interviewing-6-tips-get-you-started-heigl-mphil-phd/): 

‍

> "Instead of asking 'Would you pay for this (in the future)' ask 'Have
> you ever paid for something like this? How long? Why? What did you do
> before? Are you still using this service? If not, what made you
> stop?'  \
> \
> Asking about the solutions users found in response to a problem is
> more valuable than having to speculate about any action they may take
> in the future."

‍

By understanding how users behaved and made decisions in the past, it'll
be easier for you to predict what they might do in the future. 

‍

### 4. Make it as easy as possible for participants to schedule (and show up to) sessions.

‍

No-shows are a bummer. And, unfortunately, you can't avoid them
completely. 

‍

However, you can take steps to [reduce
no-shows](https://www.userinterviews.com/blog/how-to-reduce-no-shows-in-ux-research)
by making it as simple as possible for participants to schedule,
remember, and show up to their sessions. 

‍

Here are a few tips, drawn from Oyster's Lead UX Researcher Dr. Maria
Panagiotidi's [article on continuous user
interviews](https://uxpsychology.substack.com/p/from-no-research-to-continuous-research):

-   **Block off regular hours for sessions.** If you use a scheduling
    tool like [Calendly](https://calendly.com/) or panel management
    solution like [User
    Interviews](https://www.userinterviews.com/research-hub?source=navbarHubResearcher),
    you can create blocks of available time for sessions. Then,
    participants can simply schedule the time that works best for them,
    without back-and-forth communication to find a mutually convenient
    time for both of you. 
-   **Allow for as many different time zones as possible** if you're
    working with international users. 
-   **Schedule sessions no more than 3 weeks in advance.** More than 3
    weeks increases the chance of no-shows. If you're using a scheduling
    tool that allows you to create blocks of available time, avoid
    adding available blocks too far in the future.
-   **Send session reminders.** Using templates to save time, send
    reminder emails for the session after participants sign up, the day
    before, and the day of the interview. 

‍

All of these tips can be easily implemented with [Research
Hub](https://www.userinterviews.com/research-hub), [the \#1 recruitment
and panel management
solution](https://www.g2.com/products/user-interviews/reviews) on the
market. With Hub, approved participants can schedule sessions from a
calendar with your pre-set availability. You can also configure branding
and communication defaults for automated emails to ensure consistency
and professionalism as you remind participants of upcoming sessions. 

‍

### 5. Be an open, active, and empathetic moderator.

‍

Moderating interviews can be nerve-wracking, even for seasoned pros. If
we could sum up what it means to be a great moderator for continuous
user interviews in only 3 words, they would be:

-   **Open:** Open moderators show up to the interview with no
    expectations. They're ready to listen to whatever the participant
    wants to talk about, and they're [flexible enough to go
    off-script](https://www.userinterviews.com/blog/adapting-user-interviews-on-the-fly-sarah-merlin-of-invaluable)
    and let the conversation develop naturally.
-   **Active:** Active moderators aren't there to check boxes. They're
    attentive, engaged, and willing to [ask follow-up
    questions](https://www.userinterviews.com/blog/3-ways-to-make-a-potentially-awkward-user-interview-less-awkward-with-adam-sigel-of-hometap)
    to probe for more information. They show the participant that
    they're listening through encouraging body language such as nodding
    and eye contact. 
-   **Empathetic:** [Empathetic
    moderators](https://www.userinterviews.com/blog/active-listening-practical-empathy-babz-jewell)
    want the participant to feel safe, seen, and comfortable. They take
    the time to build rapport before diving into more sensitive topics,
    reserve judgements, and take participants seriously, even when they
    bring up details that don't seem important on the surface. 

‍

Part of being open, active, and empathetic means giving the participant
the floor---and avoiding controlling or re-directing the conversation
too much. As [Teresa Torres describes on Awkward
Silences](https://www.userinterviews.com/blog/how-to-interview-customers-continuously-with-teresa-torres-of-product-talk):

‍

> "The key is you never want to interrupt your interview participant.
> And even if what they\'re telling you is totally irrelevant, they\'re
> telling you it because they care about it. And if they care about it,
> you need to care about it."

‍

📚 For more in-depth tips on how to effectively moderate a user
interview, check out the [User Interviews
chapter](https://www.userinterviews.com/ux-research-field-guide-chapter/user-interviews)
of the Field Guide. And if you're looking for a different resource
(we're a little hurt, but we get it), GitLab also provides some great
tips for moderating interviews
[here](https://about.gitlab.com/handbook/engineering/ux/ux-research-training/facilitating-user-interviews/#tips-for-interviewing). 

‍

### 6. Follow up with participants.

‍

Just because you've asked all the questions in your interview guide
doesn't mean your work is done. 

‍

Before the interview concludes, ask the participant whether or not you
can get in touch with them in the future. This will give you the
opportunity to ask follow-up questions if they arise, address any
complaints they brought up during the interview, or update them when
you've released features that they specifically asked for. 

‍

[Ferdinand Goetzen, CEO and Co-Founder of Reveall, explains two
reasons](https://www.userinterviews.com/blog/customer-centricity-with-ferdinand-goetzen-of-reveall-podcast)
why it's important to let customers know when you've addressed a
specific complaint: 

‍

> "Number one is that the customer is happy because you fixed the
> problem, but number two, they start becoming less strict with other
> problems that might arise. What we find is that when you\'re very open
> with the customer, and if a customer really gets a feeling of, 'Hey, I
> flagged this problem and it got fixed...' They also cut you more slack
> on any potential future \[issues or requests\], be that usability
> problems, be that a bug."

‍

In other words, the simple step of following up with customers lets them
know you're listening, builds their trust, and improves customer
relationships in the future. 

‍

💰 Don't forget to follow up in the form of **incentives**, too. We
recommend distributing incentives within 10 days of the completed
session to avoid frustrating participants. 

‍

Analyzing and synthesizing continuous user interview data
---------------------------------------------------------

‍

Continuous interviews aren't just a chance for you to chit-chat with
your customers---they're also an opportunity for the rest of your team
to learn, as well. For them to do so, you need to effectively analyze
and share the insights you discover from your interviews. 

‍

📚 Your approach to analyzing continuous user interviews should be
similar to that of any qualitative data analysis you do---head to the
[Research Analysis
chapter](https://www.userinterviews.com/ux-research-field-guide-chapter/research-analysis)
to learn more. 

‍

However, one thing to keep in mind is that the feedback you receive from
continuous interviewing might be a bit more scattered than the feedback
you collect from other, more focused [research
methods](https://www.userinterviews.com/ux-research-field-guide-module/user-research-methods).
The way you organize, distill, and share your insights will make a big
difference in how and whether they're used. 

‍

Some quick tips to get you started are:

1.  Create snapshots of each individual interview.
2.  Remove personally-identifiable information.
3.  Add interview data to a research repository.
4.  Distribute snapshots and summaries widely.

‍

Let's go into detail on each. 

‍

### 1. Create snapshots of each individual interview. 

‍

Because you're interviewing continuously, you won't have a set "end"
date for your interviews, signaling the time to start compiling and
organizing data. 

‍

Instead, you need to do ongoing synthesis, ideally as soon as possible
after each interview ends. [According to Teresa
Torres](https://www.userinterviews.com/blog/how-to-interview-customers-continuously-with-teresa-torres-of-product-talk),
one great way to do this is with short, one-page interview snapshots
which provide an overview of what you learned: 

‍

> "A lot of the teams that I work with, some of them literally print
> these \[snapshots\] out and put them in a binder in their workspace,
> so that when they have a question that comes up, they can flip through
> all their past interview snapshots and look at: How often are we
> hearing this? What are the interviews we need to go back and revisit?
> Who do we want to talk to, to learn more?"

‍

These snapshots allow you to quickly summarize what you learned in
individual talks with customers and provide an easy jumping-off point
for doing a more comprehensive analysis of your vast archive of
interview data later on. 

‍

📚 Although one-page snapshots are effective and efficient for
communicating interview insights, they're not the only way. Use these
[31 creative templates and examples of UX research reports and
presentations](https://www.userinterviews.com/blog/ux-research-presentations-reports-templates-examples)
as inspiration for your next report.

‍

### 2. Remove personally-identifiable information.

‍

Keeping your participants' Personal Identifiable Information (PII)
confidential is a good practice demonstrating respect for their safety
and privacy---and it becomes an *essential* practice when your NDAs or
[consent
forms](https://www.userinterviews.com/blog/how-to-write-research-participant-consent-forms)
promise that you'll do so. 

Before you share or archive any of the insights from your interviews, be
sure to remove all PII from the files you intend to share. 

‍

For example, [here's a quick step-by-step guide to hiding the names of
research participants in
Zoom](https://www.userinterviews.com/blog/how-to-hide-participant-names-in-zoom-recordings).
This is just one step in ensuring the safe, ethical handling of
participants' data, but it's a great one to get started with. 

‍

### 3. Add interview data to a research repository. 

‍

Interview snapshots, recordings, transcripts, and other data should be
stored somewhere secure, yet accessible to others on the team. 

‍

Typically, this storage requires you to create some sort of [insights
repository](https://www.userinterviews.com/blog/librarians-on-uxr-insights-repositories-nada-alnakeeb-joanna-perez),
or a centralized database for all of your research. If you don't already
have one, taking the time to set one up might be the single most helpful
operational task you can do. 

‍

For smaller teams, this can take the form of a simple spreadsheet or an
interactive table in tools like [Airtable](http://airtable.com/),
[Notion](https://www.notion.so/), or [Trello](https://trello.com/). But
as your team and research practice scale, you'll definitely want to
switch to a more robust repository tool like
[EnjoyHQ](https://getenjoyhq.com/) or
[Dovetail](https://dovetailapp.com/). 

‍

### 4. Distribute snapshots and summaries widely. 

‍

When you've created snapshots or other types of summaries that highlight
important information from each interview, share them with the rest of
your team! The more people who know about, consume, use, and share your
insights report, the greater the impact they'll make. 

‍

If you can, try to tailor and contextualize your
[share-outs](https://www.userinterviews.com/blog/how-klaviyos-design-research-team-improved-their-critique-feedback-process)
to each team you're sharing them with. For example, engineers are likely
to be interested in specific product-related feedback, and executives
will be more likely to engage with short, easily-digestible
deliverables. 

‍

📚 Learn how to effectively communicate your findings in the [UX Research
Reports and Deliverables
chapter](https://www.userinterviews.com/ux-research-field-guide-module/research-deliverables-reporting). 

‍

What tools do you use to conduct continuous user interviews?
------------------------------------------------------------

‍

In short, the same tools you might use for regular, one-off user
interviews. These include:

-   Recruiting and panel management tools like [User
    Interviews](https://www.userinterviews.com/research-hub?source=navbarHubResearcher)
-   Video conferencing tools like [Zoom](https://zoom.com/), [Google
    Meet](https://meet.google.com/), or [Microsoft
    Teams](https://www.microsoft.com/en-us/microsoft-teams/group-chat-software)
    (or, for in-person interviews, a quiet venue)
-   Tools for recording sessions and transcription data, such as
    [Grain](https://grain.co/), [Otter.ai](https://otter.ai/), or
    [Perfect Recall](https://www.perfectrecall.app/)
-   [Qualitative research
    analysis](https://www.userinterviews.com/blog/qualitative-coding-ux-research-analysis)
    tools like
    [Aurelius](https://www.aureliuslab.com/user-research-synthesis-and-analysis),
    [Dovetail](https://dovetailapp.com/features/user-research-data-analysis/),
    or [Optimal Workshop](https://www.optimalworkshop.com/reframer/)
-   Insights management and repository tools like
    [EnjoyHQ](https://getenjoyhq.com/) or
    [Condens](https://condens.io/automated-transcription/)

‍

If you're doing other types of continuous research, you could also mix
and match tools to combine insights. For example, you could use
[Research Hub](https://www.userinterviews.com/research-hub) for building
and managing your participant panel for user interviews, then follow up
with NPS surveys at certain activation moments using tools like
[Sprig](https://sprig.com/) or [Appcues](https://www.appcues.com/). 

‍

[🧙✨ Discover more tools in the 2022 UX Research Tools
Map](https://www.userinterviews.com/ux-research-tools-map-2022), a
fantastical guide to the UXR software landscape.

‍

#### 📣 Prefer to use only *one* tool to reach both existing customers and external audiences? 

‍

You can easily automate all study logistics---including posting calls
for recruitment, pre-screening applicants, scheduling sessions, tracking
participant activity, and distributing incentives---with a comprehensive
panel management tool like [User Interviews\'s Research
Hub](https://www.userinterviews.com/research-hub). 

‍

[Visit our pricing page](https://www.userinterviews.com/pricing) to
learn more about our Free Forever, Essential, and Custom plans for every
team. 

Mixed methods for continuous research
-------------------------------------

As we mentioned earlier, continuous user interviews shouldn't replace
one-off studies or other [types of
research](https://www.userinterviews.com/ux-research-field-guide-chapter/user-research-types). 

‍

In addition to your weekly (or monthly, or quarterly---but for
ambition's sake, let's call them weekly) user interviews, use other
[qualitative and quantitative
approaches](https://www.userinterviews.com/ux-research-field-guide-chapter/qualitative-vs-quantitative-vs-mixed-methods)
to maximize insights from different audiences. 

‍

📚 To get started with other methods for continuous research, check out
the chapters on [User
Analytics](https://www.userinterviews.com/ux-research-field-guide-chapter/user-analytics),
[Continuous Feedback
Surveys](https://www.userinterviews.com/ux-research-field-guide-chapter/continuous-user-feedback-surveys),
and [Sales, Support, and Product
Data](https://www.userinterviews.com/ux-research-field-guide-chapter/integrating-support-sales-and-product-feedback).
